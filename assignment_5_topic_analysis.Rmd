---
title: "Topic 6: Topic Analysis"
author: "Felicia Cruz, Mia Forsline, Cullen Molitor, Marie Rivers"
date: '2022-05-10'
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      message = FALSE, 
                      warning = FALSE)
```

```{r packages, message=FALSE, warning=FALSE}
#install packages if necessary, then load libraries
if (!require(librarian)){
  install.packages("librarian")
  library(librarian)
}

librarian::shelf(
  forcats, 
  ggplot2,
  ggraph,
  here,
  igraph, #network plots
  kableExtra,
  ldatuning,
  LDAvis,
  lubridate,
  pdftools,
  quanteda,
  quanteda.textstats,
  quanteda.texplots,
  readr,
  readtext, #quanted subpackage for reading pdfs
  reshape2,
  stringr,
  tidyr,
  tidytext,
  tidyverse,
  tm,
  topicmodels,
  "tsne",
  widyr
  )
```

# Read in the data from PDF files in the "data" directory 

```{r}
#character of full file paths for each PDF in the data folder 
files <- list.files(path = here("data"),
                    pattern = "pdf$", full.names = TRUE)
#create a list
scripts <- lapply(files, pdf_text)

#create a readtext/dataframe of all PDF scripts 
scripts_pdf <- readtext(file = here("data", "*.pdf"),
                              docvarsfrom = c("metadata", "filenames", "filepaths"),
                              sep = "_")
```

# Create a clean corpus 
```{r}
#creating an initial corpus containing our data
script_corp <- corpus(x = scripts_pdf, text_field = "text" )

#check the corpus 
summary(script_corp) %>% 
  knitr::kable(caption = "Summary of Movie Script Corpus")
```

Add additional, context-specific stop words to stop word lexicon
```{r}
# more_stops <-c("2015",
#                "2016", 
#                "2017", 
#                "2018", 
#                "2019", 
#                "2020", 
#                "www.epa.gov", 
#                "https")
# add_stops <- tibble(word = c(stop_words$word, more_stops)) 
# stop_vec <- as_vector(add_stops)
```

Tokenize the data into single words and remove stop words 
```{r}
toks <- tokens(script_corp, 
               remove_punct = TRUE, 
               remove_numbers = TRUE)

# xxx...add custom stop words such as character names from "Don't Look Up"
add_stops <- c(stopwords("en"),"xxx", "yyy", "zzz")

toks1 <- tokens_select(toks, pattern = add_stops, selection = "remove")
```



# Convert to a document-feature matrix (dfm)
```{r}
dfm_comm <- dfm(toks1, tolower = TRUE)
dfm <- dfm_wordstem(dfm_comm)
dfm <- dfm_trim(dfm, 
                min_docfreq = 2) #keep terms that appear in at least 2 documents 

dfm %>% 
  kbl() %>% 
  kable_styling(bootstrap_options = c("striped", "hover"),
                latex_options = "HOLD_position") #hold the table position when knitting
```




```{r}
#remove rows (docs) with all zeros...for the topic model you can't have zeros
sel_idx <- slam::row_sums(dfm) > 0 
dfm <- dfm[sel_idx, ]
#comments_df <- dfm[sel_idx, ]
```

```{r}
result <- FindTopicsNumber(
  dfm,
  topics = seq(from = 2, to = 20, by = 1),
  metrics = c("CaoJuan2009",  "Deveaud2014"),
  method = "Gibbs",
  control = list(seed = 77),
  verbose = TRUE
)

FindTopicsNumber_plot(result)
```

```{r}
k <- 7 # k is the number of topics

topicModel_k7 <- LDA(dfm, k, method="Gibbs", control=list(iter = 500, verbose = 25))
```

```{r}
#nTerms(dfm_comm) 

tmResult <- posterior(topicModel_k7)
attributes(tmResult)
```

```{r}
#nTerms(dfm_comm)   
beta <- tmResult$terms   # get beta from results
dim(beta)                # K distributions over nTerms(DTM) terms# lengthOfVocab
```

```{r}
terms(topicModel_k7, 10)
```


```{r}
theta <- tmResult$topics
beta <- tmResult$terms
vocab <- (colnames(beta))
```

```{r}
comment_topics <- tidy(topicModel_k7, matrix = "beta")

top_terms <- comment_topics %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

top_terms
# beta is the probability of a term in a topic...highest beta or words most likely to be in topic
```

```{r}
top_terms %>%
  mutate(term = reorder(term, beta)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip()
```

Assign names to the topics so we know what we are working with. We can name them by their top terms

```{r}
top5termsPerTopic <- terms(topicModel_k7, 5)
topicNames <- apply(top5termsPerTopic, 2, paste, collapse=" ")
```

We can explore the theta matrix, which contains the distribution of each topic over each document

```{r}
exampleIds <- c(1, 2, 3)
N <- length(exampleIds)
```

```{r}
#lapply(epa_corp[exampleIds], as.character) #uncomment to view example text
# get topic proportions form example documents
topicProportionExamples <- theta[exampleIds,]
colnames(topicProportionExamples) <- topicNames
rownames(topicProportionExamples) <- c("Before the Flood", "Don't Look Up", "Inconvient Truth")
vizDataFrame <- melt(cbind(data.frame(topicProportionExamples), document=factor(1:N)), variable.name = "topic", id.vars = "document")  %>% 
  mutate(document = case_when(
    document == 1 ~ "Before the Flood",
    document == 2 ~ "Don't Look Up",
    document == 3 ~ "Inconvient Truth"))
ggplot(data = vizDataFrame, aes(topic, value, fill = document), ylab = "proportion") +
  geom_bar(stat="identity") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +  
  coord_flip() +
  facet_wrap(~ document, ncol = N)
# named topics based on first 5 words
# first column show prevalence of each topic in the 1st document
# 1, 2, 3 are the 1st, 2nd, and 3rd documents and the plot shows how each topic is distributed within each document
```

Hereâ€™s a neat JSON-based model visualizer

```{r}
library(LDAvis)
library("tsne")
svd_tsne <- function(x) tsne(svd(x)$u)
json <- createJSON(
  phi = tmResult$terms, 
  theta = tmResult$topics, 
  doc.length = rowSums(dfm), 
  vocab = colnames(dfm), 
  term.frequency = colSums(dfm),
  mds.method = svd_tsne,
  plot.opts = list(xlab="", ylab="")
)

serVis(json)
```

